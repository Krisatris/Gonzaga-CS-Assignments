{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ead255d7",
   "metadata": {},
   "source": [
    "# Midterm Project\n",
    "Linden Beemer - CPSC 436 - Fall 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff25a3f7",
   "metadata": {},
   "source": [
    "## 1 - Import Statements\n",
    "This project uses numpy and matplotlib in order to streamline scientific computing and plotting graphs of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a119030d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216ad8d2",
   "metadata": {},
   "source": [
    "## 2 - Dataset\n",
    "This project uses data from the University of Wisconsin on breast cancer cells. There are 569 instances in the set, each having 32 features (ID, diagnosis, and 30 real-valued input features). The goal with this data is to create a model that is 95% accurate to the actual result given."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9a3922",
   "metadata": {},
   "source": [
    "### 2.1 Loading the data\n",
    "The variables used for storing the data are as follows:\n",
    "- ``X_train`` contains 30 features detailing the mean, standard error, and the \"worst\" (largest) mean of the radius, texture, perimter, area, smoothness, compactness, concavity, concave points, symmetry, and cractal dimension of potential breast cancer cells\n",
    "- ``y_train`` is the diagnosis decision\n",
    "    - ``y_train = 1`` when a 'M' indicating malignant is seen \n",
    "    - ``y_train = 0`` when a 'B' indicating benign is seen\n",
    "- Both ``X_train`` and ``y_train`` are numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0383007b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    data = np.genfromtxt(filename, delimiter=\",\")\n",
    "    X = data[:, 2:]\n",
    "    y_data = np.genfromtxt(filename, delimiter=\",\", usecols=(1), dtype=None, encoding=None)\n",
    "    y = np.zeros(569)\n",
    "    y[y_data == 'M'] = 1.\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e220523a",
   "metadata": {},
   "source": [
    "*As a side note, I struggled to get numpy arrays to work with the mis-matched types in this data, which is why this data import looks a little odd. I'm sure I could do this in a more efficient way but this was what gave me the fewest errors importing both characters and floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "782b4433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First two elements in X_train are:\n",
      " [[1.799e+01 1.038e+01 1.228e+02 1.001e+03 1.184e-01 2.776e-01 3.001e-01\n",
      "  1.471e-01 2.419e-01 7.871e-02 1.095e+00 9.053e-01 8.589e+00 1.534e+02\n",
      "  6.399e-03 4.904e-02 5.373e-02 1.587e-02 3.003e-02 6.193e-03 2.538e+01\n",
      "  1.733e+01 1.846e+02 2.019e+03 1.622e-01 6.656e-01 7.119e-01 2.654e-01\n",
      "  4.601e-01 1.189e-01]\n",
      " [2.057e+01 1.777e+01 1.329e+02 1.326e+03 8.474e-02 7.864e-02 8.690e-02\n",
      "  7.017e-02 1.812e-01 5.667e-02 5.435e-01 7.339e-01 3.398e+00 7.408e+01\n",
      "  5.225e-03 1.308e-02 1.860e-02 1.340e-02 1.389e-02 3.532e-03 2.499e+01\n",
      "  2.341e+01 1.588e+02 1.956e+03 1.238e-01 1.866e-01 2.416e-01 1.860e-01\n",
      "  2.750e-01 8.902e-02]]\n",
      "Type of X_train: <class 'numpy.ndarray'>\n",
      "First five elements in y_train are:\n",
      " [1. 1. 1. 1. 1.]\n",
      "Type of y_train: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# load and view some of the dataset\n",
    "\n",
    "X_train, y_train = load_data(\"./data/wdbc.data\")\n",
    "\n",
    "print(\"First two elements in X_train are:\\n\", X_train[:2])\n",
    "print(\"Type of X_train:\",type(X_train))\n",
    "\n",
    "print(\"First five elements in y_train are:\\n\", y_train[:5])\n",
    "print(\"Type of y_train:\",type(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8d0d17b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X_train is: (569, 30)\n",
      "The shape of y_train is: (569,)\n",
      "We have m = 569 training examples\n"
     ]
    }
   ],
   "source": [
    "print(\"The shape of X_train is: \" + str(X_train.shape))\n",
    "print(\"The shape of y_train is: \" + str(y_train.shape))\n",
    "print(\"We have m = %d training examples\" % (len(y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d150648",
   "metadata": {},
   "source": [
    "## 3 - Z-Score normalization\n",
    "Z-score normalization gives all features in the table a mean of 0 and a standard deviation of 1. This makes it less likely that certain features are given more weight than others when modeled. This code is adapted from homework 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "452ddc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore_normalize_features(X):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X (ndarray (m, n))      : input data, m examples, n features\n",
    "        \n",
    "    Returns:\n",
    "        X_norm (ndarray (m, n)) : input normalized by column\n",
    "        mu (ndarray (n,))       : mean of each feature\n",
    "        sigma (ndarray (n,))    : standard deviation of each feature\n",
    "    \"\"\"\n",
    "    mu = np.mean(X, axis=0)\n",
    "    sigma = np.std(X, axis=0)\n",
    "    X_norm = (X - mu) / sigma\n",
    "    \n",
    "    return(X_norm, mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0f16c533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_mu = [1.41272917e+01 1.92896485e+01 9.19690334e+01 6.54889104e+02\n",
      " 9.63602812e-02], \n",
      "X_sigma = [3.52095076e+00 4.29725464e+00 2.42776193e+01 3.51604754e+02\n",
      " 1.40517641e-02]\n",
      "\n",
      "first two normalized datapoints:\n",
      " [[ 1.09706398e+00 -2.07333501e+00  1.26993369e+00  9.84374905e-01\n",
      "   1.56846633e+00  3.28351467e+00  2.65287398e+00  2.53247522e+00\n",
      "   2.21751501e+00  2.25574689e+00  2.48973393e+00 -5.65265059e-01\n",
      "   2.83303087e+00  2.48757756e+00 -2.14001647e-01  1.31686157e+00\n",
      "   7.24026158e-01  6.60819941e-01  1.14875667e+00  9.07083081e-01\n",
      "   1.88668963e+00 -1.35929347e+00  2.30360062e+00  2.00123749e+00\n",
      "   1.30768627e+00  2.61666502e+00  2.10952635e+00  2.29607613e+00\n",
      "   2.75062224e+00  1.93701461e+00]\n",
      " [ 1.82982061e+00 -3.53632408e-01  1.68595471e+00  1.90870825e+00\n",
      "  -8.26962447e-01 -4.87071673e-01 -2.38458552e-02  5.48144156e-01\n",
      "   1.39236330e-03 -8.68652457e-01  4.99254601e-01 -8.76243603e-01\n",
      "   2.63326966e-01  7.42401948e-01 -6.05350847e-01 -6.92926270e-01\n",
      "  -4.40780058e-01  2.60162067e-01 -8.05450380e-01 -9.94437403e-02\n",
      "   1.80592744e+00 -3.69203222e-01  1.53512599e+00  1.89048899e+00\n",
      "  -3.75611957e-01 -4.30444219e-01 -1.46748968e-01  1.08708430e+00\n",
      "  -2.43889668e-01  2.81189987e-01]]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_mu, X_sigma = zscore_normalize_features(X_train)\n",
    "print(f\"X_mu = {X_mu[:5]}, \\nX_sigma = {X_sigma[:5]}\\n\")\n",
    "print(\"first two normalized datapoints:\\n\", X_train[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bd55a7",
   "metadata": {},
   "source": [
    "## 4 - Normalization in computing cost and gradient descent\n",
    "Because there are so many features present in this dataset, it is important we minimize the weight of certain ones in order to prevent overfitting. We do this by using a specialized cost and gradient descent equation. I will be using a logistic regression model, with an added regularization parameter in the computing cost and gradient descent functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b15add",
   "metadata": {},
   "source": [
    "### 4.1 - Sigmoid function implementation\n",
    "I will be implementing the sigmoid function I wrote in homework 5 to shorten my code and make it more readable. Evaluating this ```sigmoid(0)``` should give me exactly 0.5, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9815d08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Args:\n",
    "        z (ndarray): A scalar, numpy array of any size.\n",
    "\n",
    "    Returns:\n",
    "        g (ndarray): sigmoid(z), with the same shape as z\n",
    "         \n",
    "    \"\"\"\n",
    "          \n",
    "    if(isinstance(z, int) or isinstance(z, float)):\n",
    "        g = 1 / (1 + np.exp(-1 * z))\n",
    "    else:\n",
    "        n = z.shape[0]\n",
    "        g = np.zeros(n)\n",
    "        for i in range(n):\n",
    "            g[i] = 1 / (1 + np.exp(-1 * z[i])) \n",
    "    \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "06b84eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid(0) = 0.5\n"
     ]
    }
   ],
   "source": [
    "print (\"sigmoid(0) = \" + str(sigmoid(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2952cc7b",
   "metadata": {},
   "source": [
    "### 4.2 - Logisitic Cost Function with Regularization\n",
    "Here, I implement the compute_cost function from homework 5, this time making use of the lambda placeholder that was present but unused in the last homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3a7b0f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w, b, lambda_):\n",
    "    \"\"\"\n",
    "    Computes the cost over all examples\n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) data, m examples by n features\n",
    "      y : (array_like Shape (m,)) target value \n",
    "      w : (array_like Shape (n,)) Values of parameters of the model      \n",
    "      b : scalar Values of bias parameter of the model\n",
    "      lambda_: (scalar) normalization parameter\n",
    "    Returns:\n",
    "      total_cost: (scalar)         cost \n",
    "    \"\"\"\n",
    "\n",
    "    m, n = X.shape\n",
    "    total_cost = 0\n",
    "    curr_cost = 0\n",
    "    regularized_constant = 0\n",
    "    \n",
    "    # compute initial logistic regression cost\n",
    "    for i in range(m):\n",
    "        f_wb = sigmoid(np.dot(X[i], w) + b)\n",
    "        curr_cost = ((-1 * y[i] * np.log(f_wb)) - \n",
    "                       (1 - y[i]) * np.log((1 - f_wb)))\n",
    "        \n",
    "        # calculate the lambda constant\n",
    "        regularized_constant = 0\n",
    "        for j in range(n):\n",
    "            regularized_constant += w[j] ** 2\n",
    "        regularized_constant = (lambda_ * regularized_constant) / (2 * m)\n",
    "        \n",
    "        total_cost += (curr_cost + regularized_constant)\n",
    "    \n",
    "    total_cost = -1 * total_cost / m\n",
    "    \n",
    "\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ffa933",
   "metadata": {},
   "source": [
    "Here, I will run my compute_cost with an initial w and b equal to 0. This is more to help me with troubleshooting any errors that might arise as zeroes will not change the regularized constant section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3cdd40e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at initial w (zeros): -0.693\n"
     ]
    }
   ],
   "source": [
    "m, n = X_train.shape\n",
    "\n",
    "# Compute and display cost with w initialized to zeroes\n",
    "initial_w = np.zeros(n)\n",
    "initial_b = 0.\n",
    "initial_lambda = 1.\n",
    "cost = compute_cost(X_train, y_train, initial_w, initial_b, initial_lambda)\n",
    "print('Cost at initial w (zeros): {:.3f}'.format(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3430918e",
   "metadata": {},
   "source": [
    "### 4.3 - Regularized gradient for logistic regression\n",
    "Here, I implement the compute_gradient function from homework 5, this time making use of the lambda placeholder that was present but unused in the last homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aa1e70fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w, b, lambda_): \n",
    "    \"\"\"\n",
    "    Computes the gradient for logistic regression \n",
    " \n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) variable such as house size \n",
    "      y : (array_like Shape (m,1)) actual value \n",
    "      w : (array_like Shape (n,1)) values of parameters of the model      \n",
    "      b : (scalar)                 value of parameter of the model \n",
    "      lambda_: (scalar) normalization parameter\n",
    "    Returns\n",
    "      dj_dw: (array_like Shape (n,1)) The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db: (scalar)                The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    dj_dw = np.zeros(w.shape)\n",
    "    dj_db = 0.\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb = sigmoid(np.dot(X[i], w) + b)\n",
    "        dj_db += (f_wb - y[i])\n",
    "        for j in range(n):\n",
    "            dj_dw[j] += ((f_wb - y[i]) * X[i][j]) + ((lambda_ / m) * w[j])\n",
    "            \n",
    "    dj_dw = dj_dw / m\n",
    "    dj_db = dj_db / m\n",
    "\n",
    "        \n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056d93d5",
   "metadata": {},
   "source": [
    "Here, I will run my compute_gradient with an initial w and b equal to 0. This is more to help me with troubleshooting any errors that might arise as zeroes will not change the regularized constant section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3c93dc5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dj_db at initial w (zeros):0.1274165202108963\n",
      "dj_dw at initial w (zeros):[-0.3529633348145923, -0.20073899267749487, -0.35905873406226524, -0.3427883916743645, -0.17336106608943685, -0.2884195793200141, -0.33668471935543093, -0.3754869934056591, -0.15979358346446093, 0.006206885058401443, -0.2742049681145695, 0.004014599499701369, -0.2688898779301959, -0.26506798396292136, 0.03240174076973867, -0.14166294704487786, -0.12267644749050105, -0.1972854214005769, 0.003153220271648589, -0.03769908166157324, -0.3754096049015078, -0.22090910288224022, -0.3785331400409052, -0.3547989256038203, -0.20377511364437365, -0.2857432355691958, -0.31891661202522464, -0.38368324447763913, -0.20127519131440294, -0.1565897851978686]\n"
     ]
    }
   ],
   "source": [
    "# Compute and display gradient with w initialized to zeroes\n",
    "initial_w = np.zeros(n)\n",
    "initial_b = 0.\n",
    "initial_lambda = 1.\n",
    "\n",
    "dj_db, dj_dw = compute_gradient(X_train, y_train, initial_w, initial_b, initial_lambda)\n",
    "print(f'dj_db at initial w (zeros):{dj_db}' )\n",
    "print(f'dj_dw at initial w (zeros):{dj_dw.tolist()}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61fd18e",
   "metadata": {},
   "source": [
    "### 4.3 - Learning parameters using gradient descent\n",
    "Here, I implement the gradient_descent function present in homework 5. No edits are needed to this function as the regularization is implemented in previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e99f11c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters, lambda_): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn theta. Updates theta by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      X :    (array_like Shape (m, n)\n",
    "      y :    (array_like Shape (m,))\n",
    "      w_in : (array_like Shape (n,))  Initial values of parameters of the model\n",
    "      b_in : (scalar)                 Initial value of parameter of the model\n",
    "      cost_function:                  function to compute cost\n",
    "      alpha : (float)                 Learning rate\n",
    "      num_iters : (int)               number of iterations to run gradient descent\n",
    "      lambda_ (scalar, float)         regularization constant\n",
    "      \n",
    "    Returns:\n",
    "      w : (array_like Shape (n,)) Updated values of parameters of the model after\n",
    "          running gradient descent\n",
    "      b : (scalar)                Updated value of parameter of the model after\n",
    "          running gradient descent\n",
    "    \"\"\"\n",
    "    \n",
    "    # number of training examples\n",
    "    m = len(X)\n",
    "    \n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w_history = []\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db, dj_dw = compute_gradient(X, y, w_in, b_in, lambda_)   \n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w_in = w_in - (alpha * dj_dw)               \n",
    "        b_in = b_in - (alpha * dj_db)              \n",
    "       \n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            cost =  cost_function(X, y, w_in, b_in, lambda_)\n",
    "            J_history.append(cost)\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters/10) == 0 or i == (num_iters-1):\n",
    "            w_history.append(w_in)\n",
    "            print(f\"Iteration {i:4}: Cost {float(J_history[-1]):8.2f}   \")\n",
    "        \n",
    "    return w_in, b_in, J_history, w_history #return w and J,w history for graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d0aa7a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost    -0.66   \n",
      "Iteration 1000: Cost    -0.24   \n",
      "Iteration 2000: Cost    -0.18   \n",
      "Iteration 3000: Cost    -0.15   \n",
      "Iteration 4000: Cost    -0.14   \n",
      "Iteration 5000: Cost    -0.13   \n",
      "Iteration 6000: Cost    -0.12   \n",
      "Iteration 7000: Cost    -0.12   \n",
      "Iteration 8000: Cost    -0.11   \n",
      "Iteration 9000: Cost    -0.11   \n",
      "Iteration 9999: Cost    -0.10   \n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "intial_w = 0.01 * (np.random.rand(2).reshape(-1,1) - 0.5)\n",
    "initial_b = -0.5\n",
    "\n",
    "\n",
    "# Some gradient descent settings\n",
    "iterations = 10000\n",
    "alpha = 0.001\n",
    "lambda_ = 0.01\n",
    "\n",
    "w,b, J_history,_ = gradient_descent(X_train ,y_train, initial_w, initial_b, \n",
    "                                   compute_cost, compute_gradient, alpha, iterations, lambda_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84554d26",
   "metadata": {},
   "source": [
    "## 5 - Visualizing the learning curve\n",
    "Below is the learning curve of iterations via cost as the compute_gradient function runs. This uses the code defined in homework 4 to visualize the learning curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c15f6103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cost_versus_iteration(hist):\n",
    "    plt.plot(hist, c='b',label='Cost')\n",
    "    plt.title(\"Cost vs. iteration\"); \n",
    "    plt.ylabel('Cost')             ;  \n",
    "    plt.xlabel('iteration step')   ;  \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "34cbe817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAiRElEQVR4nO3de5xVdb3/8ddbLiICIldHBUcTLTRFJTXNNAWOoYX6K7vpwV+n6HI6dcqOD81+p6xT6fE8+lmPbpJd+CWZpqlknhRRMi+JgKIEGoImCMKAci3Fgc/vj++aM3s2e2ZvmNmz9sy8n4/HeqzLXnutz3cY9nu+a+21liICMzOztuyVdwFmZlb7HBZmZlaWw8LMzMpyWJiZWVkOCzMzK8thYWZmZTkszDqZpNGStkrqlWMNH5F0b177t67HYWFdhqQPS5qffdCukfTfkt7Rzm2+IGlCR9VYiYh4MSIGRMSOrIa5kj5Wrf1JqpcUknoX1DAzIiZVa5/W/TgsrEuQ9AXgOuCbwEhgNPADYEqOZdWEPHso1nM4LKzmSdoP+BrwzxHxm4jYFhFvRMRvI+LfsnX2lnSdpNXZcJ2kvbPXhkm6S9JGSa9I+qOkvST9ghQ6v816K5eV2PdSSecWzPeWtF7S8ZL6SbpR0oZs249LGllBe/7nL31J3wBOA76X1fC9bJ03S5qd1fuspAsL3v9zST+UdLekbcC7JJ0j6QlJmyWtlPTVgl0+mI03Zvt4u6RLJD1UsM1Tsvo3ZeNTCl6bK+nrkh6WtEXSvZKGlf+Xs24lIjx4qOkBOBtoBHq3sc7XgD8BI4DhwCPA17PXvgX8COiTDacByl57AZjQxnb/HZhZMH8O8Ew2/Qngt0B/oBdwAjCogvbUA9HUHmAu8LGC1/cFVgL/G+gNHA+sB47KXv85sAk4lfQHXz/gDOCt2fwxwFrgvFL7y5ZdAjyUTQ8BXgUuzvb3oWx+aEF9y4EjgH2y+avz/r3w0LmDexbWFQwF1kdEYxvrfAT4WkSsi4gG4CrShx/AG0AdcEikHskfI6LSm6L9EnivpP7Z/IezZU3bHQocHhE7ImJBRGzejXa15lzghYj4WUQ0RsRC4DbgfQXr3BkRD0fEzoh4LSLmRsTT2fxTwE3A6RXu7xxgWUT8ItvfTcAzwHsK1vlZRPwlIv4O3AKMa28jrWtxWFhXsAEYVniCtoQDgb8WzP81WwZwLfAccK+kFZIur3THEfEcsBR4TxYY76U5LH4B3AP8Kjv09Z+S+lS67TYcApyUHdraKGkjKQwPKFhnZeEbJJ0k6QFJDZI2AZ8EKj1UVPyzI5s/qGD+5YLpvwEDKty2dRMOC+sKHgVeA85rY53VpA/ZJqOzZUTEloi4NCIOI/21/AVJZ2XrVdLDuIl0aGYKsCQLELJeylURMRY4hdQj+MeKW9WsuIaVwB8iYnDBMCAiPtXGe34JzAJGRcR+pMNuamXdYsU/O0g/v5cqboF1ew4Lq3kRsYl07uD7ks6T1F9SH0nvlvSf2Wo3AV+WNDw7+frvwI0Aks6VdLgkAZuBHdkA6dj+YWVK+BUwCfgUzb0KJL1L0luzbyNtJh2W2lF6E20qruEu4AhJF2ft7CPpbZLe0sY2BgKvRMRrkk4kHS5r0gDspPV23p3t78PZSfcPAGOzOswAh4V1ERHxbeALwJdJH34rgc8Ad2Sr/AcwH3gKeBpYmC0DGAPcB2wl9VJ+EBFzs9e+RQqZjZK+2Mq+12TvOwW4ueClA4BbSUGxFPgDzQH1I0k/qrB53wHeJ+lVSd+NiC2kcPog6a/+l4FrgL3b2Manga9J2kIKylsK6v8b8A3g4aydJxe1bwOpV3Qp6ZDfZcC5EbG+wvqtB2j6RoiZmVmr3LMwM7OyHBZmZlaWw8LMzMpyWJiZWVltXeTUZQ0bNizq6+vzLsPMrMtYsGDB+ogY3trr3TIs6uvrmT9/ft5lmJl1GZKKr+JvwYehzMysLIeFmZmV5bAwM7OyHBZmZlZWLmEhaUj2FLBl2Xj/Vtb7qaR1khZ3do1mZtYsr57F5cCciBgDzMnmS/k56SlpZmaWo7zCYgowI5ueQSvPKYiIB4FXOqkmMzNrRV7XWYzMbvtMRKyRNKK9G5Q0DZgGMHr06PZuzswsNxGwfTv87W9p+Pvfm6fbmu/bFy67rDo1VS0sJN1Hy8dANrmyGvuLiOnAdIDx48f7vutmVlXbt8O2baWHrVtbf614ndY+/Hfu3P2aDjigC4ZFRExo7TVJayXVZb2KOmBdteows54tIn0Ab9nScti8ueX81q3lP+QLX29s3L069tkH9t1316GuDvr3T6/379887Ml8n454Anwr8joMNQuYClydje/MqQ4zq0ER6YN540bYtGnXD/pSH/atvb51a+V/pZf6QB8wAA46qPQHfeE6bb3evz/06lXVH1nV5RUWVwO3SPon4EXg/QCSDgRuiIjJ2fxNwBnAMEmrgK9ExE/yKdnMKhEBr72WPuSbPuwrnW6a37y5sg/4Pn1g0CAYOLB52H9/GD265bKmoXjdwmHffbv+B3o15RIW2TN/zyqxfDUwuWD+Q51Zl5k1e/11ePVVeOWVXYdSy5s+9DduhDfeaHvbe+0F++3XPAweDIcc0jxduHzQoNY/5Pdu66nk1qG65V1nzazZzp3pA7yhAdavT+OGhtIhUBgE27a1vs299oIhQ9Jf8UOGwLBhMGZM6Q/7UtMDBoDUOe23juGwMOtiGhtbfugXThcOTcvXr4cdO0pvq29fGDo0feAPGQKHHgrHH98839owcGAKDOs5HBZmNSAi/TX/8svNw5o1Leebhg0bWt/O/vvD8OFpOPxwePvbm+eHDWueHj48hcQ++/gvfKuMw8KsippCYNWq5uGll3YNhLVrSx/n79cvfbXygAPgiCPgne+EESNafugXfvj39v9oqxL/apntoZ0704f8Sy+1DIPiYHjttZbvk9IHflMIHH10GpcaBg3yX/5WGxwWZq3YsSP95f/CC/D882lcOLz44q4XZvXpAwcfnIa3vQ3OP795vmkYOdI9AOt6/CtrPdrGjbBsWRpWrNg1DIoPDdXVQX09nHQSXHghjBrVMgiGDfOJX+ueHBbW7W3alMLgueeag6Fpfv36lus2hcGJJ6YwqK9vHkaPTucQzHoih4V1CxGwejUsWdJyePbZ9PXRQqNGpWsCLrggjZuGQw9N3w4ys105LKxLiUgnjRcvbg6EP/85jTdvbl5vyBA46ig477wUBIcfnsZvepMDwWxPOCysZjU2pp7Bk0+2HAoPHY0YAWPHwkUXpfFRR6Xx8OH+FpFZR3JYWE1obISnn4Z582D+/BQKTz+d7k8E6R5ARx8NU6bAuHFwzDEpFIYNy7Nqs57DYWGdLiJ90+ixx5qHhQvTA18gHUI67jj4zGdSMIwbB0ceWd179ZtZ2xwWVnWNjbBoETz4IPzxj/DII+liNkjfLjr+ePjEJ9LXUU86KX3zyIeQzGqLw8I63Ouvp0NJDz6YhocfTg+hgXSC+R/+oTkYjjnGPQazrsBhYe0WAc88A/feC/fcA3/4Q3qMJaQTzhddlO5pdNpp6YljZtb1OCxsj7z6KsyenQLi3nth5cq0/Igj4KMfhTPPTOHgE9Bm3YPDwiq2ciXceSfccUfqPTQ2pgfZTJgAX/4yTJqUzjeYWffjsLA2LV0Kt96aAmLhwrTszW+GSy+F97wnnXfwTfHMuj//N7ddvPgi/OpXcNNN6XoHKT1E55pr0nUORx6Zd4Vm1tkcFgakm+3ddBPMnAkPPZSWnXwyfOc78P73pxvsmVnP5bDowSLg0Ufhxz+Gm29OF8WNHQv/8R/woQ/BYYflXaGZ1QqHRQ+0eTP8/Odw/fXpBnwDBsDFF8PHPw4nnOAL4sxsVw6LHuSvf4XvfhduuCEFxoknpl7FBz+YAsPMrDUOix5gwYJ0cvq221Kv4cIL4fOfT4/9NDOrhMOiG5s/H666Cu66K10P8cUvppvzjRqVd2Vm1tU4LLqhBQvgK1+B3/0u3cH1G99IITFoUN6VmVlXlcuj5SUNkTRb0rJsvH+JdUZJekDSUkl/lvS5PGrtSlauTCeqx49P33L6xjfg+efhS19yUJhZ++QSFsDlwJyIGAPMyeaLNQKXRsRbgJOBf5Y0thNr7DK2bEm32zjiCPj1r+Hyy2HFCoeEmXWcvMJiCjAjm54BnFe8QkSsiYiF2fQWYCnge5YWueMOeMtbUi/iggvSY0i/9a10jsLMrKPkFRYjI2INpFAARrS1sqR64DjgsTbWmSZpvqT5DQ0NHVlrTVq1Cs4/Pw1Dh6bDTjNnwiGH5F2ZmXVHVTvBLek+4IASL125m9sZANwG/GtEbG5tvYiYDkwHGD9+fOzOPrqSCJgxAz772XTX12uuSV+D9QOEzKyaqhYWETGhtdckrZVUFxFrJNUB61pZrw8pKGZGxG+qVGqXsWFDevzobbfB6afDT3/qW3KYWefI6zDULGBqNj0VuLN4BUkCfgIsjYhvd2JtNWnu3PQI0lmzUm9izhwHhZl1nrzC4mpgoqRlwMRsHkkHSro7W+dU4GLgTElPZsPkfMrNTwR8+9vpAUMDB8Kf/gSXXQa9euVdmZn1JLlclBcRG4CzSixfDUzOph8CevQt7bZtg499LD1b4oIL0s3/Bg7Muyoz64l8BXeNWrsWzjknPZ3um99M1074brBmlheHRQ36y1/g7LNTYMyaBeeem3dFZtbTOSxqzOOPw7vfnXoRDzyQbiNuZpa3vE5wWwnz5sHEiekWHY8+6qAws9rhnkWNmDcPJk1Kd4mdOxdGj867IjOzZu5Z1IBFixwUZlbbHBY5e/FFmDw5PdbUQWFmtcqHoXL0yivpW0/btsEf/+igMLPa5bDISWMjvP/9sHw53HMPvPWteVdkZtY6h0VOvvQluP9++NnP4Iwz8q7GzKxtPmeRg1//Gq69Fj71KbjkkryrMTMrz2HRyVasgI9+FE4+Ga67Lu9qzMwq47DoRI2NcPHF6Y6xN98MffvmXZGZWWV8zqITXXMNPPII3Hijv/lkZl2LexadZNEi+OpX4QMfgA9/OO9qzMx2j8OiE+zcCZ/8JAweDN//vm81bmZdjw9DdYIbbkhPuJsxA4YOzbsaM7Pd555FlTU0pAcXnX56OrltZtYVOSyq7KqrYPNm+MEPfPjJzLouh0UVPfccXH89fPzjMHZs3tWYme05h0UVXXllupbiK1/JuxIzs/ZxWFTJggVwyy1w6aVwwAF5V2Nm1j4Oiyr55jfTV2W/+MW8KzEzaz+HRRUsXQq33w6f+Ux6nraZWVfnsKiCa66BffaBz30u70rMzDqGw6KDrVwJM2fCtGkwbFje1ZiZdYxcwkLSEEmzJS3LxvuXWKefpHmSFkn6s6Sr8qh1d02fDjt2uFdhZt1LXj2Ly4E5ETEGmJPNF3sdODMijgXGAWdLOrnzStx927fDj38M554L9fV5V2Nm1nHyCospwIxsegZwXvEKkWzNZvtkQ3RKdXvo9tth7Vr49KfzrsTMrGPlFRYjI2INQDYeUWolSb0kPQmsA2ZHxGOtbVDSNEnzJc1vaGioRs1l/eAHcNhhMGlSLrs3M6uaqoWFpPskLS4xTKl0GxGxIyLGAQcDJ0o6uo11p0fE+IgYP3z48A5owe557jl48MF0Ynsvf23AzLqZqt2iPCImtPaapLWS6iJijaQ6Us+hrW1tlDQXOBtY3LGVdoyZM9ONAj/ykbwrMTPreHn9DTwLmJpNTwXuLF5B0nBJg7PpfYAJwDOdVeDuiEiPSn3Xu+Dgg/Ouxsys4+UVFlcDEyUtAyZm80g6UNLd2Tp1wAOSngIeJ52zuCuXasuYNy8dhrroorwrMTOrjlyelBcRG4CzSixfDUzOpp8Cjuvk0vbIjTdCv35wwQV5V2JmVh0+FdtOEekrs2efDfvtl3c1ZmbV4bBopwUL4KWX4Lzz8q7EzKx6HBbtdOed6auy55yTdyVmZtXjsGinO+6A007zTQPNrHtzWLTDihWweLEPQZlZ9+ewaIff/z6Nzz033zrMzKrNYdEOs2enu8u+6U15V2JmVl0Oiz3U2AgPPAATJqTbfJiZdWcOiz00fz5s2gQTJ+ZdiZlZ9Tks9tB996UexZln5l2JmVn1OSz20H33wbhx/sqsmfUMDos9sH07PPYYnH563pWYmXUOh8UeePJJeO01OPXUvCsxM+scDos98MgjaXzKKfnWYWbWWRwWe+Dhh+GQQ+DAA/OuxMysc1QUFpJ+UcmyniAi9Sx8CMrMepJKexZHFc5I6gWc0PHl1L4XX4TVq30Iysx6ljbDQtIVkrYAx0janA1bgHWUeG52TzBvXhqfdFK+dZiZdaY2wyIivhURA4FrI2JQNgyMiKERcUUn1VhTFi6E3r3hrW/NuxIzs85T6WGouyTtCyDpIknflnRIFeuqWU88AUcfDXvvnXclZmadp9Kw+CHwN0nHApcBfwX+X9WqqlERqWdx3HF5V2Jm1rkqDYvGiAhgCvCdiPgOMLB6ZdWm1auhoQGOPz7vSszMOlfvCtfbIukK4GLgtOzbUH2qV1ZtWrgwjd2zMLOeptKexQeA14GPRsTLwEHAtVWrqkYtXJjuNHvssXlXYmbWuSoKiywgZgL7SToXeC0ietw5i0WLYMwYGDAg70rMzDpXpVdwXwjMA94PXAg8Jul91SysFi1ZAkcdVX49M7PuptJzFlcCb4uIdQCShgP3AbdWq7Bas307PPccvK/HRaSZWeXnLPZqCorMht147y4kDZE0W9KybLx/G+v2kvSEpLv2dH8dYdky2LEDxo7Nswozs3xU+oH/e0n3SLpE0iXA74C727Hfy4E5ETEGmJPNt+ZzwNJ27KtDLM0qeMtb8q3DzCwP5e4NdbikUyPi34DrgWOAY4FHgent2O8UYEY2PQM4r5X9HwycA9zQjn11iCVL0jehjjwy70rMzDpfuZ7FdcAWgIj4TUR8ISI+T+pVXNeO/Y6MiDXZdtcAI9rY/2XAznIblDRN0nxJ8xsaGtpRWmlLl0J9PfTv3+GbNjOreeVOcNdHxFPFCyNivqT6tt4o6T7ggBIvXVlJYdlXdNdFxAJJZ5RbPyKmk/V2xo8fH5XsY3csWeLzFWbWc5ULi35tvLZPW2+MiAmtvSZpraS6iFgjqY50y/NipwLvlTQ5q2OQpBsj4qIyNXe4nTvhL3+BSZM6e89mZrWh3GGoxyV9vHihpH8CFrRjv7OAqdn0VEo8GyMiroiIgyOiHvggcH8eQQHw0kvw2mvpgjwzs56oXM/iX4HbJX2E5nAYD/QFzm/Hfq8GbslC50XSxX5IOhC4ISImt2PbHW758jQ+7LB86zAzy0ubYRERa4FTJL0LODpb/LuIuL89O42IDcBZJZavBnYJioiYC8xtzz7bY8WKNH7Tm/KqwMwsXxVdwR0RDwAPVLmWmrV8eXo63qhReVdiZpaPPb4KuydZvhwOOSQFhplZT+SwqMCKFT4EZWY9m8OiAsuX++S2mfVsDosyNm6EV15xz8LMejaHRRlN34Ryz8LMejKHRRkvvJDG9fV5VmFmli+HRRmrVqWxvzZrZj2Zw6KMlSuhXz8YNizvSszM8uOwKGPVKjj44PQsCzOznsphUcbKlSkszMx6ModFGStX+nyFmZnDog07dsDq1e5ZmJk5LNqwdi00NrpnYWbmsGhD09dm3bMws57OYdGGlSvT2D0LM+vpHBZt8AV5ZmaJw6INq1dD374wZEjelZiZ5cth0Ya1a2HkSF+QZ2bmsGjDyy/DAQfkXYWZWf4cFm1o6lmYmfV0Dos2uGdhZpY4LFqxYwc0NLhnYWYGDotWbdiQAsM9CzMzh0Wr1q5NY/cszMwcFq16+eU0dliYmUHvPHYqaQhwM1APvABcGBGvlljvBWALsANojIjxnVVjU8/Ch6HMzPLrWVwOzImIMcCcbL4174qIcZ0ZFOCehZlZobzCYgowI5ueAZyXUx2tWrs2PXt70KC8KzEzy19eYTEyItYAZOMRrawXwL2SFkia1mnV4Vt9mJkVqto5C0n3AaWO+F+5G5s5NSJWSxoBzJb0TEQ82Mr+pgHTAEaPHr3b9RZbvx6GDWv3ZszMuoWqhUVETGjtNUlrJdVFxBpJdcC6VraxOhuvk3Q7cCJQMiwiYjowHWD8+PHR3vo3bIChQ9u7FTOz7iGvw1CzgKnZ9FTgzuIVJO0raWDTNDAJWNxZBToszMya5RUWVwMTJS0DJmbzSDpQ0t3ZOiOBhyQtAuYBv4uI33dWgQ4LM7NmuVxnEREbgLNKLF8NTM6mVwDHdnJpADQ2wsaNDgszsya+gruEV7PLAx0WZmaJw6KEDRvS2GFhZpY4LEpwWJiZteSwKMFhYWbWksOiBIeFmVlLDosSHBZmZi05LErYsAF694aBA/OuxMysNjgsSmi6IM83ETQzSxwWJfjqbTOzlhwWJTgszMxacliUsHEj7L9/3lWYmdUOh0UJmzbBfvvlXYWZWe1wWJTgsDAza8lhUSQihcXgwXlXYmZWOxwWRbZuhZ073bMwMyvksCiycWMaOyzMzJo5LIps2pTGPgxlZtbMYVGkKSzcszAza+awKNJ0GMo9CzOzZg6LIu5ZmJntymFRxGFhZrYrh0URH4YyM9uVw6LIpk3Qty/065d3JWZmtcNhUcS3+jAz25XDosjGjT4EZWZWzGFRxD0LM7NdOSyKOCzMzHaVS1hIGiJptqRl2bjko4YkDZZ0q6RnJC2V9PZq1+bDUGZmu8qrZ3E5MCcixgBzsvlSvgP8PiLeDBwLLK12YVu2wMCB1d6LmVnXkldYTAFmZNMzgPOKV5A0CHgn8BOAiNgeERurXZjDwsxsV3mFxciIWAOQjUeUWOcwoAH4maQnJN0gad/WNihpmqT5kuY3NDTsUVER6XkWDgszs5aqFhaS7pO0uMQwpcJN9AaOB34YEccB22j9cBURMT0ixkfE+OHDh+9Rzdu3Q2MjDBiwR283M+u2eldrwxExobXXJK2VVBcRayTVAetKrLYKWBURj2Xzt9JGWHSELVvS2GFhZtZSXoehZgFTs+mpwJ3FK0TEy8BKSUdmi84CllSzqK1b09hhYWbWUl5hcTUwUdIyYGI2j6QDJd1dsN6/ADMlPQWMA75ZzaKawsLnLMzMWqraYai2RMQGUk+hePlqYHLB/JPA+M6qyz0LM7PSfAV3AZ+zMDMrzWFRwD0LM7PSHBYFfM7CzKw0h0UBH4YyMyvNYVHAh6HMzEpzWBTYuhUk6N8/70rMzGqLw6LA1q2pVyHlXYmZWW1xWBTYssWHoMzMSnFYFGjqWZiZWUsOiwIOCzOz0hwWBfzgIzOz0hwWBdyzMDMrzWFRwGFhZlaaw6KAw8LMrDSHRQGfszAzK81hUeA974ETTsi7CjOz2pPLw49q1Y035l2BmVltcs/CzMzKcliYmVlZDgszMyvLYWFmZmU5LMzMrCyHhZmZleWwMDOzshwWZmZWliIi7xo6nKQG4K97+PZhwPoOLKcrcJu7v57WXnCbd9chETG8tRe7ZVi0h6T5ETE+7zo6k9vc/fW09oLb3NF8GMrMzMpyWJiZWVkOi11Nz7uAHLjN3V9Pay+4zR3K5yzMzKws9yzMzKwsh4WZmZXlsMhIOlvSs5Kek3R53vW0h6RRkh6QtFTSnyV9Lls+RNJsScuy8f4F77kia/uzkv6hYPkJkp7OXvuuJOXRpkpI6iXpCUl3ZfPdvb2DJd0q6Zns3/rtPaDNn89+pxdLuklSv+7WZkk/lbRO0uKCZR3WRkl7S7o5W/6YpPqKCouIHj8AvYDlwGFAX2ARMDbvutrRnjrg+Gx6IPAXYCzwn8Dl2fLLgWuy6bFZm/cGDs1+Fr2y1+YBbwcE/Dfw7rzb10a7vwD8Ergrm+/u7Z0BfCyb7gsM7s5tBg4Cngf2yeZvAS7pbm0G3gkcDywuWNZhbQQ+Dfwom/4gcHNFdeX9g6mFIfuB3lMwfwVwRd51dWD77gQmAs8CddmyOuDZUu0F7sl+JnXAMwXLPwRcn3d7WmnjwcAc4Eyaw6I7t3dQ9sGpouXduc0HASuBIaRHQt8FTOqObQbqi8Kiw9rYtE423Zt0xbfK1eTDUEnTL2GTVdmyLi/rYh4HPAaMjIg1ANl4RLZaa+0/KJsuXl6LrgMuA3YWLOvO7T0MaAB+lh16u0HSvnTjNkfES8B/AS8Ca4BNEXEv3bjNBTqyjf/znohoBDYBQ8sV4LBISh2v7PLfKZY0ALgN+NeI2NzWqiWWRRvLa4qkc4F1EbGg0reUWNZl2pvpTTpU8cOIOA7YRjo80Zou3+bsOP0U0uGWA4F9JV3U1ltKLOtSba7AnrRxj9rvsEhWAaMK5g8GVudUS4eQ1IcUFDMj4jfZ4rWS6rLX64B12fLW2r8qmy5eXmtOBd4r6QXgV8CZkm6k+7YXUq2rIuKxbP5WUnh05zZPAJ6PiIaIeAP4DXAK3bvNTTqyjf/zHkm9gf2AV8oV4LBIHgfGSDpUUl/SSZ9ZOde0x7JvPfwEWBoR3y54aRYwNZueSjqX0bT8g9m3JA4FxgDzsu7uFkknZ9v8x4L31IyIuCIiDo6IetK/3f0RcRHdtL0AEfEysFLSkdmis4AldOM2kw4/nSypf1brWcBSunebm3RkGwu39T7S/5fyPau8T+TUygBMJn1raDlwZd71tLMt7yB1K58CnsyGyaTjknOAZdl4SMF7rsza/iwF3wwBxgOLs9e+RwUnwnJu+xk0n+Du1u0FxgHzs3/nO4D9e0CbrwKeyer9BelbQN2qzcBNpHMyb5B6Af/UkW0E+gG/Bp4jfWPqsErq8u0+zMysLB+GMjOzshwWZmZWlsPCzMzKcliYmVlZDgszMyvLYWE9iqRHsnG9pA938La/VGpf1SDpDEmnVGv7ZsUcFtajRETTB2w9sFthIalXmVVahEXBvqrhDNLVy2adwmFhPYqkrdnk1cBpkp7MnpHQS9K1kh6X9JSkT2Trn6H0bJBfAk9ny+6QtCB7rsK0bNnVwD7Z9mYW7kvJtUrPYHha0gcKtj1Xzc+kmFnquQqSPitpSVbXr7KbQ34S+Hy2v9MkDZd0W1b/45JOzd77VUm/kHS/0rMQPl7FH691Z3lfrejBQ2cOwNZsfAbZld7Z/DTgy9n03qQrow/N1tsGHFqw7pBsvA/pCtmhhdsusa//BcwmPTdlJOm2FXXZtjeR7tuzF/Ao8I4SNa8G9s6mB2fjrwJfLFjnl03vBUaTbvXStN6irNZhpLuNHpj3v4OHrjf0bn/cmHULk4BjJL0vm9+PdJ+d7aR77TxfsO5nJZ2fTY/K1tvQxrbfAdwUETtIN4T7A/A2YHO27VUAkp4kHR57qOj9TwEzJd1Buq1HKROAsQUdk0GSBmbTd0bE34G/S3oAOLGN7ZiV5LAwSwT8S0Tc02KhdAapZ1E4P4H08Ji/SZpLutdOuW235vWC6R2U/j95Dunpae8F/o+ko0qss1dW09+L6oddbz/te/zYbvM5C+uptpAeOdvkHuBT2a3dkXSE0sOEiu0HvJoFxZuBkwtee6Pp/UUeBD6QnRcZTvrgn1dJkZL2AkZFxAOkhzsNBgaUqP9e4DMF7xtX8NoUpWdVDyUd+nq8kn2bFXJYWE/1FNAoaZGkzwM3kG7xvVDSYuB6Sv+V/3ugt6SngK8Dfyp4bTrwVNMJ7gK3Z/tbBNwPXBbpFuOV6AXcKOlp4Ang/0bERuC3wPlNJ7iBzwLjs5PgS0gnwJvMA36X1fr1iKj1ZzdYDfJdZ826MUlfJZ1o/6+8a7GuzT0LMzMryz0LMzMryz0LMzMry2FhZmZlOSzMzKwsh4WZmZXlsDAzs7L+P4aMyS8txKgdAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_cost_versus_iteration(J_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fded34",
   "metadata": {},
   "source": [
    "## 6 - Prediction\n",
    "Here, I take previous work from homework 5 to predict whether a given instance is malignant (1) or benign (0). This uses the w and b computed during gradient descent to make a prediction and predicts 0 if the result is less than 0.5, 1 if greater."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b2c70661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w, b): \n",
    "    \"\"\"\n",
    "    Predict whether the label is 0 or 1 using learned logistic\n",
    "    regression parameters w\n",
    "    \n",
    "    Args:\n",
    "    X : (ndarray Shape (m, n))\n",
    "    w : (array_like Shape (n,))      Parameters of the model\n",
    "    b : (scalar, float)              Parameter of the model\n",
    "\n",
    "    Returns:\n",
    "    p: (ndarray (m,1))\n",
    "        The predictions for X using a threshold at 0.5\n",
    "    \"\"\"\n",
    "    # number of training examples\n",
    "    m, n = X.shape   \n",
    "    p = np.zeros(m)\n",
    "   \n",
    "    for i in range(m):\n",
    "        f_wb = sigmoid(np.dot(X[i], w) + b)\n",
    "        p[i] = 0 if (f_wb < 0.5) else 1\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2783e2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 97.539543\n"
     ]
    }
   ],
   "source": [
    "#Compute accuracy on our training set\n",
    "p = predict(X_train, w,b)\n",
    "print('Train Accuracy: %f'%(np.mean(p == y_train) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf9d327",
   "metadata": {},
   "source": [
    "As you can see, after gradient descent my model has a 97.5% accuracy rate. This is also what the authors of the dataset estimate as their model's accuracy, so I am confident in it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
