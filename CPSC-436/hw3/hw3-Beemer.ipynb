{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f30ab37d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1cc1814c",
   "metadata": {},
   "source": [
    "## Goals\n",
    "- Review the features of NumPy and Python that are used in this course\n",
    "- automate the process of optimizing $w$ and $b$ using gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0c4cdd",
   "metadata": {},
   "source": [
    "## Tools\n",
    "In this assignment, we will make use of: \n",
    "- NumPy, a popular library for scientific computing\n",
    "- Matplotlib, a popular library for plotting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1318a8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ab1296",
   "metadata": {},
   "source": [
    "# 1. Python, NumPy and Arrays\n",
    "A brief introduction to some of the scientific computing used in this course. In particular the NumPy scientific computing package and its use with python.\n",
    "\n",
    "### Useful References\n",
    "- NumPy Documentation including a basic introduction: [NumPy.org](https://NumPy.org/doc/stable/)\n",
    "- A challenging feature topic: [NumPy Broadcasting](https://NumPy.org/doc/stable/user/basics.broadcasting.html)\n",
    "\n",
    "### Python and NumPy \n",
    "Python is the programming language we will be using in this course. It has a set of numeric data types and arithmetic operations. NumPy is a library that extends the base capabilities of python to add a richer data set including more numeric types, vectors, matrices, and many matrix functions. NumPy and python  work together fairly seamlessly. Python arithmetic operators work on NumPy data types and many NumPy functions will accept python data types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695883ab",
   "metadata": {},
   "source": [
    "## 1.1 Vectors\n",
    "\n",
    "<img align=\"right\" src=\"./images/Vectors.PNG\" style=\"width:340px;\" >Vectors, as you will use them in this course, are ordered arrays of numbers. In notation, vectors are denoted with lower case bold letters such as $\\mathbf{x}$.  The elements of a vector are all the same type. A vector does not, for example, contain both characters and numbers. The number of elements in the array is often referred to as the *dimension* though mathematicians may prefer *rank*. The vector shown has a dimension of $n$. The elements of a vector can be referenced with an index. In math settings, indexes typically run from 1 to n. In Python, indexing will run from 0 to n-1.  In notation, elements of a vector, when referenced individually will indicate the index in a subscript, for example, the $0^{th}$ element, of the vector $\\mathbf{x}$ is $x_0$. Note, the x is not bold in this case.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ec09d7",
   "metadata": {},
   "source": [
    "### NumPy Arrays\n",
    "\n",
    "NumPy's basic data structure is an indexable, n-dimensional *array* containing elements of the same type (`dtype`). Right away, you may notice we have overloaded the term 'dimension'. Above, it was the number of elements in the vector, here, dimension refers to the number of indexes of an array. A one-dimensional or 1-D array has one index. In this course, we will represent vectors as NumPy 1-D arrays. \n",
    "\n",
    " - 1-D array, shape (n,): n elements indexed [0] through [n-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f7015d",
   "metadata": {},
   "source": [
    "### Vector Creation\n",
    "\n",
    "Data creation routines in NumPy will generally have a first parameter which is the shape of the object. This can either be a single value for a 1-D result or a tuple (n,m,...) specifying the shape of the result. Below are examples of creating vectors using these routines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3807b727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.zeros(4) :   a = [0. 0. 0. 0.], a shape = (4,), a data type = float64\n",
      "np.zeros(4,) :  a = [0. 0. 0. 0.], a shape = (4,), a data type = float64\n",
      "np.random.random_sample(4): a = [0.93344321 0.69120776 0.36780364 0.5246332 ], a shape = (4,), a data type = float64\n"
     ]
    }
   ],
   "source": [
    "# NumPy routines which allocate memory and fill arrays with value\n",
    "a = np.zeros(4);                print(f\"np.zeros(4) :   a = {a}, a shape = {a.shape}, a data type = {a.dtype}\")\n",
    "a = np.zeros((4,));             print(f\"np.zeros(4,) :  a = {a}, a shape = {a.shape}, a data type = {a.dtype}\")\n",
    "a = np.random.random_sample(4); print(f\"np.random.random_sample(4): a = {a}, a shape = {a.shape}, a data type = {a.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b2ebca",
   "metadata": {},
   "source": [
    "Some data creation routines do not take a shape tuple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81a0369e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.arange(4.):     a = [0. 1. 2. 3.], a shape = (4,), a data type = float64\n",
      "np.random.rand(4): a = [0.7478983  0.16369919 0.04233716 0.63183064], a shape = (4,), a data type = float64\n"
     ]
    }
   ],
   "source": [
    "# NumPy routines which allocate memory and fill arrays with value but do not accept shape as input argument\n",
    "a = np.arange(4.);              print(f\"np.arange(4.):     a = {a}, a shape = {a.shape}, a data type = {a.dtype}\")\n",
    "a = np.random.rand(4);          print(f\"np.random.rand(4): a = {a}, a shape = {a.shape}, a data type = {a.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cc4528",
   "metadata": {},
   "source": [
    "Values can be specified manually as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1e9459f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.array([5,4,3,2]):  a = [5 4 3 2],     a shape = (4,), a data type = int32\n",
      "np.array([5.,4,3,2]): a = [5. 4. 3. 2.], a shape = (4,), a data type = float64\n"
     ]
    }
   ],
   "source": [
    "# NumPy routines which allocate memory and fill with user specified values\n",
    "a = np.array([5,4,3,2]);  print(f\"np.array([5,4,3,2]):  a = {a},     a shape = {a.shape}, a data type = {a.dtype}\")\n",
    "a = np.array([5.,4,3,2]); print(f\"np.array([5.,4,3,2]): a = {a}, a shape = {a.shape}, a data type = {a.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645a447e",
   "metadata": {},
   "source": [
    "These have all created a one-dimensional vector  `a` with four elements. `a.shape` returns the dimensions. Here we see a.shape = `(4,)` indicating a 1-d array with 4 elements. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8447353",
   "metadata": {},
   "source": [
    "\n",
    "### Opetions on Vectors\n",
    "Let's explore some operations using vectors.\n",
    "\n",
    "### 1) Indexing\n",
    "Elements of vectors can be accessed via indexing and slicing. NumPy provides a very complete set of indexing and slicing capabilities. We will explore only the basics needed for the course here. Reference [Slicing and Indexing](https://NumPy.org/doc/stable/reference/arrays.indexing.html) for more details.  \n",
    "**Indexing** means referring to *an element* of an array by its position within the array.  \n",
    "**Slicing** means getting a *subset* of elements from an array based on their indices.  \n",
    "NumPy starts indexing at zero so the 3rd element of an vector $\\mathbf{a}$ is `a[2]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f3c7371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "a[2].shape: () a[2]  = 2, Accessing an element returns a scalar\n",
      "a[-1] = 9\n",
      "The error message you'll see is:\n",
      "index 10 is out of bounds for axis 0 with size 10\n"
     ]
    }
   ],
   "source": [
    "#vector indexing operations on 1-D vectors\n",
    "a = np.arange(10)\n",
    "print(a)\n",
    "\n",
    "#access an element\n",
    "print(f\"a[2].shape: {a[2].shape} a[2]  = {a[2]}, Accessing an element returns a scalar\")\n",
    "\n",
    "# access the last element, negative indexes count from the end\n",
    "print(f\"a[-1] = {a[-1]}\")\n",
    "\n",
    "#indexs must be within the range of the vector or they will produce and error\n",
    "try:\n",
    "    c = a[10]\n",
    "except Exception as e:\n",
    "    print(\"The error message you'll see is:\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb39757",
   "metadata": {},
   "source": [
    "### 2) Slicing\n",
    "Slicing creates an array of indices using a set of three values (`start:stop:step`). A subset of values is also valid. Its use is best explained by example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46f5484a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a         = [0 1 2 3 4 5 6 7 8 9]\n",
      "a[2:7:1] =  [2 3 4 5 6]\n",
      "a[2:7:2] =  [2 4 6]\n",
      "a[3:]    =  [3 4 5 6 7 8 9]\n",
      "a[:3]    =  [0 1 2]\n",
      "a[:]     =  [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "#vector slicing operations\n",
    "a = np.arange(10)\n",
    "print(f\"a         = {a}\")\n",
    "\n",
    "#access 5 consecutive elements (start:stop:step)\n",
    "c = a[2:7:1];     print(\"a[2:7:1] = \", c)\n",
    "\n",
    "# access 3 elements separated by two \n",
    "c = a[2:7:2];     print(\"a[2:7:2] = \", c)\n",
    "\n",
    "# access all elements index 3 and above\n",
    "c = a[3:];        print(\"a[3:]    = \", c)\n",
    "\n",
    "# access all elements below index 3\n",
    "c = a[:3];        print(\"a[:3]    = \", c)\n",
    "\n",
    "# access all elements\n",
    "c = a[:];         print(\"a[:]     = \", c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d60163",
   "metadata": {},
   "source": [
    "## 1.2 Matrices\n",
    "\n",
    "Matrices, are two dimensional arrays. The elements of a matrix are all of the same type. In notation, matrices are denoted with capitol, bold letter such as $\\mathbf{X}$. In this and other assignments, `m` is often the number of rows and `n` the number of columns. The elements of a matrix can be referenced with a two dimensional index. In math settings, numbers in the index typically run from 1 to n. In Python, indexing will run from 0 to n-1.  \n",
    "<figure>\n",
    "    <center> <img src=\"./images/Matrices.PNG\"  alt='missing'  width=900><center/>\n",
    "    <figcaption> Generic Matrix Notation, 1st index is row, 2nd is column </figcaption>\n",
    "<figure/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73a4a75",
   "metadata": {},
   "source": [
    "### NumPy Arrays\n",
    "\n",
    "NumPy's basic data structure is an indexable, n-dimensional *array* containing elements of the same type (`dtype`). These were described earlier. Matrices have a two-dimensional (2-D) index [m,n].\n",
    "\n",
    "In this course, 2-D matrices are used to hold training data. Training data is $m$ examples by $n$ features creating an (m,n) array. This assignment does not do operations directly on matrices but typically extracts an example as a vector and operates on that. Below you will review: \n",
    "- data creation\n",
    "- slicing and indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a592e84c",
   "metadata": {},
   "source": [
    "### Matrix Creation\n",
    "The same functions that created 1-D vectors will create 2-D or n-D arrays. Here are some examples\n",
    "\n",
    "Below, the shape tuple is provided to achieve a 2-D result. Notice how NumPy uses brackets to denote each dimension. Notice further than NumPy, when printing, will print one row per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9685be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a shape = (1, 5), a = [[0. 0. 0. 0. 0.]]\n",
      "a shape = (2, 1), a = [[0.]\n",
      " [0.]]\n",
      "a shape = (1, 1), a = [[0.27042222]]\n"
     ]
    }
   ],
   "source": [
    "a = np.zeros((1, 5))                                       \n",
    "print(f\"a shape = {a.shape}, a = {a}\")                     \n",
    "\n",
    "a = np.zeros((2, 1))                                                                   \n",
    "print(f\"a shape = {a.shape}, a = {a}\") \n",
    "\n",
    "a = np.random.random_sample((1, 1))  \n",
    "print(f\"a shape = {a.shape}, a = {a}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea492bf",
   "metadata": {},
   "source": [
    "One can also manually specify data. Dimensions are specified with additional brackets matching the format in the printing above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "728cab59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a shape = (3, 1), np.array: a = [[5]\n",
      " [4]\n",
      " [3]]\n",
      " a shape = (3, 1), np.array: a = [[5]\n",
      " [4]\n",
      " [3]]\n"
     ]
    }
   ],
   "source": [
    "# NumPy routines which allocate memory and fill with user specified values\n",
    "a = np.array([[5], [4], [3]]);   print(f\" a shape = {a.shape}, np.array: a = {a}\")\n",
    "a = np.array([[5],   # One can also\n",
    "              [4],   # separate values\n",
    "              [3]]); #into separate rows\n",
    "print(f\" a shape = {a.shape}, np.array: a = {a}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec7befe",
   "metadata": {},
   "source": [
    "### Operations on Matrices\n",
    "Let's explore some operations using matrices.\n",
    "\n",
    "### 1) Indexing\n",
    "\n",
    "Matrices include a second index. The two indexes describe [row, column]. Access can either return an element or a row/column. See below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be4270f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.shape: (3, 2), \n",
      "a= [[0 1]\n",
      " [2 3]\n",
      " [4 5]]\n",
      "\n",
      "a[2,0].shape:   (), a[2,0] = 4,     type(a[2,0]) = <class 'numpy.int32'> Accessing an element returns a scalar\n",
      "\n",
      "a[2].shape:   (2,), a[2]   = [4 5], type(a[2])   = <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#vector indexing operations on matrices\n",
    "a = np.arange(6).reshape(-1, 2)   #reshape is a convenient way to create matrices\n",
    "print(f\"a.shape: {a.shape}, \\na= {a}\")\n",
    "\n",
    "#access an element\n",
    "print(f\"\\na[2,0].shape:   {a[2, 0].shape}, a[2,0] = {a[2, 0]},     type(a[2,0]) = {type(a[2, 0])} Accessing an element returns a scalar\\n\")\n",
    "\n",
    "#access a row\n",
    "print(f\"a[2].shape:   {a[2].shape}, a[2]   = {a[2]}, type(a[2])   = {type(a[2])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e1ead3",
   "metadata": {},
   "source": [
    "It is worth drawing attention to the last example. Accessing a matrix by just specifying the row will return a *1-D vector*.\n",
    "\n",
    "**Reshape**  \n",
    "The previous example used [reshape](https://numpy.org/doc/stable/reference/generated/numpy.reshape.html) to shape the array.  \n",
    "`a = np.arange(6).reshape(-1, 2) `   \n",
    "This line of code first created a *1-D Vector* of six elements. It then reshaped that vector into a *2-D* array using the reshape command. This could have been written:  \n",
    "`a = np.arange(6).reshape(3, 2) `  \n",
    "To arrive at the same 3 row, 2 column array.\n",
    "The -1 argument tells the routine to compute the number of rows given the size of the array and the number of columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c314fcc9",
   "metadata": {},
   "source": [
    "### 2) Slicing\n",
    "\n",
    "Slicing creates an array of indices using a set of three values (`start:stop:step`). A subset of values is also valid. Its use is best explained by example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f40b8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = \n",
      "[[ 0  1  2  3  4  5  6  7  8  9]\n",
      " [10 11 12 13 14 15 16 17 18 19]]\n",
      "a[0, 2:7:1] =  [2 3 4 5 6] ,  a[0, 2:7:1].shape = (5,) a 1-D array\n",
      "a[:, 2:7:1] = \n",
      " [[ 2  3  4  5  6]\n",
      " [12 13 14 15 16]] ,  a[:, 2:7:1].shape = (2, 5) a 2-D array\n",
      "a[:,:] = \n",
      " [[ 0  1  2  3  4  5  6  7  8  9]\n",
      " [10 11 12 13 14 15 16 17 18 19]] ,  a[:,:].shape = (2, 10)\n",
      "a[1,:] =  [10 11 12 13 14 15 16 17 18 19] ,  a[1,:].shape = (10,) a 1-D array\n",
      "a[1]   =  [10 11 12 13 14 15 16 17 18 19] ,  a[1].shape   = (10,) a 1-D array\n"
     ]
    }
   ],
   "source": [
    "#vector 2-D slicing operations\n",
    "a = np.arange(20).reshape(-1, 10)\n",
    "print(f\"a = \\n{a}\")\n",
    "\n",
    "#access 5 consecutive elements (start:stop:step)\n",
    "print(\"a[0, 2:7:1] = \", a[0, 2:7:1], \",  a[0, 2:7:1].shape =\", a[0, 2:7:1].shape, \"a 1-D array\")\n",
    "\n",
    "#access 5 consecutive elements (start:stop:step) in two rows\n",
    "print(\"a[:, 2:7:1] = \\n\", a[:, 2:7:1], \",  a[:, 2:7:1].shape =\", a[:, 2:7:1].shape, \"a 2-D array\")\n",
    "\n",
    "# access all elements\n",
    "print(\"a[:,:] = \\n\", a[:,:], \",  a[:,:].shape =\", a[:,:].shape)\n",
    "\n",
    "# access all elements in one row (very common usage)\n",
    "print(\"a[1,:] = \", a[1,:], \",  a[1,:].shape =\", a[1,:].shape, \"a 1-D array\")\n",
    "# same as\n",
    "print(\"a[1]   = \", a[1],   \",  a[1].shape   =\", a[1].shape, \"a 1-D array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c28b43b",
   "metadata": {},
   "source": [
    "# 2. Gradient Descent for Linear Regression\n",
    "\n",
    "## 2.1 Problem Statement\n",
    "\n",
    "Let's use the same two data points as before - the example of diabetes progression prediction\n",
    "\n",
    "| BMI                      | Diabetes progression |\n",
    "| -------------------------| ------------------------ |\n",
    "| 32.1                   | 151                      |\n",
    "| 21.6                   | 75                      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0cb80725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our data set\n",
    "x_train = np.array([32.1, 21.6]) #features\n",
    "y_train = np.array([151, 75])    #target value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d77081",
   "metadata": {},
   "source": [
    "## 2.2 Compute_Cost\n",
    "This was developed in the last assignment. We'll need it again here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2c986fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate the cost\n",
    "def compute_cost(x, y, w, b):\n",
    "   \n",
    "    m = x.shape[0] # Number of training examples\n",
    "    cost = 0\n",
    "    \n",
    "    for i in range(m):\n",
    "        f_wb = w * x[i] + b\n",
    "        cost = cost + (f_wb - y[i])**2\n",
    "    total_cost = 1 / (2 * m) * cost\n",
    "\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9363ed79",
   "metadata": {},
   "source": [
    "\n",
    "## 2.3 Gradient descent summary\n",
    "So far in this course, you have developed a linear model that predicts $f_{w,b}(x^{(i)})$:\n",
    "$$f_{w,b}(x^{(i)}) = wx^{(i)} + b \\tag{1}$$\n",
    "In linear regression, you utilize input training data to fit the parameters $w$,$b$ by minimizing a measure of the error between our predictions $f_{w,b}(x^{(i)})$ and the actual data $y^{(i)}$. The measure is called the $cost$, $J(w,b)$. In training you measure the cost over all of our training samples $x^{(i)},y^{(i)}$\n",
    "$$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2\\tag{2}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59add658",
   "metadata": {},
   "source": [
    "\n",
    "In lecture, *gradient descent* was described as:\n",
    "\n",
    "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\n",
    "\\;  w &= w -  \\alpha \\frac{\\partial J(w,b)}{\\partial w} \\tag{3}  \\; \\newline \n",
    " b &= b -  \\alpha \\frac{\\partial J(w,b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "where, parameters $w$, $b$ are updated simultaneously.  \n",
    "The gradient is defined as:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(w,b)}{\\partial w}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)} \\tag{4}\\\\\n",
    "  \\frac{\\partial J(w,b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \\tag{5}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Here *simultaniously* means that you calculate the partial derivatives for all the parameters before updating any of the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4cb374",
   "metadata": {},
   "source": [
    "## 2.4 Implement Gradient Descent\n",
    "You will implement gradient descent algorithm for one feature. You will need three functions. \n",
    "- `compute_gradient` implementing equation (4) and (5) above\n",
    "- `compute_cost` implementing equation (2) above (code from previous assignment)\n",
    "- `gradient_descent`, utilizing compute_gradient and compute_cost\n",
    "\n",
    "Conventions:\n",
    "- The naming of python variables containing partial derivatives follows this pattern,$\\frac{\\partial J(w,b)}{\\partial b}$  will be `dj_db`.\n",
    "- w.r.t is With Respect To, as in partial derivative of $J(wb)$ With Respect To $b$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3899c8",
   "metadata": {},
   "source": [
    "<a name=\"toc_40291_2.3\"></a>\n",
    "### compute_gradient\n",
    "<a name='ex-01'></a>\n",
    "`compute_gradient`  implements (4) and (5) above and returns $\\frac{\\partial J(w,b)}{\\partial w}$,$\\frac{\\partial J(w,b)}{\\partial b}$. The embedded comments describe the operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a360ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(x, y, w, b): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      x (ndarray (m,)): Data, m examples \n",
    "      y (ndarray (m,)): target values\n",
    "      w,b (scalar)    : model parameters  \n",
    "    Returns\n",
    "      dj_dw (scalar): The gradient of the cost w.r.t. the parameters w\n",
    "      dj_db (scalar): The gradient of the cost w.r.t. the parameter b     \n",
    "     \"\"\"\n",
    "    \n",
    "    # Number of training examples\n",
    "    m = x.shape[0]    \n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "\n",
    "    # Write for loop to get the sum\n",
    "    for i in range(m):  \n",
    "        dj_dw += ((w * x[i] + b) - y[i]) * x[i]\n",
    "        dj_db += (w * x[i] + b) - y[i]\n",
    "        \n",
    "        \n",
    "        \n",
    "    dj_dw = dj_dw / m \n",
    "    dj_db = dj_db / m \n",
    "        \n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d95a77",
   "metadata": {},
   "source": [
    "Now test your compute_gradient function with $w$=1 and $b$=1 using x_train and y_train. You should get dj_dw: -2458.215\n",
    "dj_db: -85.15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cf0fcd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dj_dw: -2458.215\n",
      "dj_db: -85.15\n"
     ]
    }
   ],
   "source": [
    "w = 1\n",
    "b = 1\n",
    "dj_dw, dj_db = compute_gradient(x_train, y_train, w, b)      # call compute_gradient\n",
    "print(f\"dj_dw: {dj_dw}\")\n",
    "print(f\"dj_db: {dj_db}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103d1a11",
   "metadata": {},
   "source": [
    "###  Gradient Descent\n",
    "Now that gradients can be computed,  gradient descent, described in equation (3) above can be implemented below in `gradient_descent`. The details of the implementation are described in the comments. Below, you will utilize this function to find optimal values of $w$ and $b$ on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8be11695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, w_in, b_in, alpha, num_iters, cost_function, gradient_function): \n",
    "    \"\"\"\n",
    "    Performs gradient descent to fit w,b. Updates w,b by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      x (ndarray (m,))  : Data, m examples \n",
    "      y (ndarray (m,))  : target values\n",
    "      w_in,b_in (scalar): initial values of model parameters  \n",
    "      alpha (float):     Learning rate\n",
    "      num_iters (int):   number of iterations to run gradient descent\n",
    "      cost_function:     function to call to produce cost\n",
    "      gradient_function: function to call to produce gradient\n",
    "      \n",
    "    Returns:\n",
    "      w (scalar): Updated value of parameter after running gradient descent\n",
    "      b (scalar): Updated value of parameter after running gradient descent\n",
    "      J_history (List): History of cost values\n",
    "      p_history (list): History of parameters [w,b] \n",
    "      \"\"\"\n",
    "    \n",
    "    w = copy.deepcopy(w_in) # avoid modifying global w_in\n",
    "    # An array to store cost J and w's at each iteration primarily for checking history later\n",
    "    J_history = []\n",
    "    p_history = []\n",
    "    b = b_in\n",
    "    w = w_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # Calculate the gradient and update the parameters using gradient_function\n",
    "        dj_dw, dj_db = gradient_function(x, y, w, b)     \n",
    "\n",
    "        # Write code to update Parameters using equation (3) above\n",
    "        b = b - (alpha * dj_db)                          \n",
    "        w = w - (alpha * dj_dw)                           \n",
    "\n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            J_history.append(cost_function(x, y, w , b))\n",
    "            p_history.append([w,b])\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters/10) == 0:\n",
    "            print(f\"Iteration {i:4}: Cost {J_history[-1]:0.2e} \",\n",
    "                  f\"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}  \",\n",
    "                  f\"w: {w: 0.3e}, b:{b: 0.5e}\")\n",
    " \n",
    "    return w, b, J_history, p_history #return w and J,w history for graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d90195aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost 5.61e+02  dj_dw: -3.234e+03, dj_db: -1.130e+02   w:  3.234e+00, b: 1.13000e-01\n",
      "Iteration 10000: Cost 5.86e+01  dj_dw: -7.444e-02, dj_db:  2.075e+00   w:  5.214e+00, b:-2.49265e+01\n",
      "Iteration 20000: Cost 2.81e+01  dj_dw: -5.153e-02, dj_db:  1.436e+00   w:  5.837e+00, b:-4.22874e+01\n",
      "Iteration 30000: Cost 1.35e+01  dj_dw: -3.567e-02, dj_db:  9.944e-01   w:  6.268e+00, b:-5.43059e+01\n",
      "Iteration 40000: Cost 6.45e+00  dj_dw: -2.470e-02, dj_db:  6.884e-01   w:  6.567e+00, b:-6.26259e+01\n",
      "Iteration 50000: Cost 3.09e+00  dj_dw: -1.710e-02, dj_db:  4.765e-01   w:  6.773e+00, b:-6.83857e+01\n",
      "Iteration 60000: Cost 1.48e+00  dj_dw: -1.183e-02, dj_db:  3.299e-01   w:  6.916e+00, b:-7.23730e+01\n",
      "Iteration 70000: Cost 7.10e-01  dj_dw: -8.193e-03, dj_db:  2.284e-01   w:  7.015e+00, b:-7.51333e+01\n",
      "Iteration 80000: Cost 3.40e-01  dj_dw: -5.672e-03, dj_db:  1.581e-01   w:  7.084e+00, b:-7.70441e+01\n",
      "Iteration 90000: Cost 1.63e-01  dj_dw: -3.926e-03, dj_db:  1.094e-01   w:  7.131e+00, b:-7.83670e+01\n",
      "(w,b) found by gradient descent: (  7.1642,-79.2827)\n"
     ]
    }
   ],
   "source": [
    "# initialize parameters\n",
    "w_init = 0\n",
    "b_init = 0\n",
    "# some gradient descent settings\n",
    "iterations = 100000\n",
    "tmp_alpha = 1.0e-3\n",
    "\n",
    "# call gradient descent\n",
    "w_final, b_final, J_hist, p_hist = gradient_descent(x_train, y_train, w_init, b_init, \n",
    "                                                    tmp_alpha, iterations, compute_cost, compute_gradient)\n",
    "\n",
    "print(f\"(w,b) found by gradient descent: ({w_final:8.4f},{b_final:8.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9645edd0",
   "metadata": {},
   "source": [
    "Take a moment and note some characteristics of the gradient descent process printed above.  \n",
    "\n",
    "- The cost starts large and rapidly declines as described in the slide from the lecture.\n",
    "- The partial derivatives, `dj_dw`, and `dj_db` also get smaller, rapidly at first and then more slowly. As shown in the diagram from the lecture, as the process nears the 'bottom of the bowl' progress is slower due to the smaller value of the derivative at that point.\n",
    "- progress slows though the learning rate, alpha, remains fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79750c32",
   "metadata": {},
   "source": [
    "### Cost versus iterations of gradient descent \n",
    "A plot of cost versus iterations is a useful measure of progress in gradient descent. Cost should always decrease in successful runs. The change in cost is so rapid initially, it is useful to plot the initial decent on a different scale than the final descent. In the plots below, note the scale of cost on the axes and the iteration step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a5261e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot cost versus iteration  \n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(12,4))\n",
    "ax1.plot(J_hist[:1000])\n",
    "ax2.plot(10000 + np.arange(len(J_hist[10000:])), J_hist[10000:])\n",
    "ax1.set_title(\"Cost vs. iteration(start)\");  ax2.set_title(\"Cost vs. iteration (end)\")\n",
    "ax1.set_ylabel('Cost')            ;  ax2.set_ylabel('Cost') \n",
    "ax1.set_xlabel('iteration step')  ;  ax2.set_xlabel('iteration step') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7740c879",
   "metadata": {},
   "source": [
    "### Predictions\n",
    "Now that you have discovered the optimal values for the parameters $w$ and $b$, you can now use the model to predict housing values based on our learned parameters. As expected, the predicted values are nearly the same as the training values for the same housing. Further, the value not in the prediction is in line with the expected value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce76c68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BMI=30.5 diabetes progression prediction 139.2\n",
      "BMI=22.6 diabetes progression prediction 82.6\n"
     ]
    }
   ],
   "source": [
    "print(f\"BMI=30.5 diabetes progression prediction {w_final*30.5 + b_final:0.1f}\")\n",
    "print(f\"BMI=22.6 diabetes progression prediction {w_final*22.6 + b_final:0.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ebb87e",
   "metadata": {},
   "source": [
    "### Increased Learning Rate\n",
    "\n",
    "In the lecture, there was a discussion related to the proper value of the learning rate, $\\alpha$ in equation(3). The larger $\\alpha$ is, the faster gradient descent will converge to a solution. But, if it is too large, gradient descent will diverge. Above you have an example of a solution which converges nicely.\n",
    "\n",
    "Let's try increasing the value of  $\\alpha$ and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "082cf81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost 5.28e+04  dj_dw: -3.234e+03, dj_db: -1.130e+02   w:  1.617e+01, b: 5.65000e-01\n",
      "Iteration    1: Cost 3.98e+05  dj_dw:  8.883e+03, dj_db:  3.217e+02   w: -2.825e+01, b:-1.04335e+00\n",
      "Iteration    2: Cost 3.00e+06  dj_dw: -2.440e+04, dj_db: -8.725e+02   w:  9.377e+01, b: 3.31902e+00\n",
      "Iteration    3: Cost 2.27e+07  dj_dw:  6.704e+04, dj_db:  2.408e+03   w: -2.414e+02, b:-8.72159e+00\n",
      "Iteration    4: Cost 1.71e+08  dj_dw: -1.842e+05, dj_db: -6.604e+03   w:  6.795e+02, b: 2.43008e+01\n",
      "Iteration    5: Cost 1.29e+09  dj_dw:  5.060e+05, dj_db:  1.816e+04   w: -1.851e+03, b:-6.64757e+01\n",
      "Iteration    6: Cost 9.74e+09  dj_dw: -1.390e+06, dj_db: -4.987e+04   w:  5.100e+03, b: 1.82853e+02\n",
      "Iteration    7: Cost 7.35e+10  dj_dw:  3.819e+06, dj_db:  1.370e+05   w: -1.399e+04, b:-5.02169e+02\n",
      "Iteration    8: Cost 5.55e+11  dj_dw: -1.049e+07, dj_db: -3.764e+05   w:  3.846e+04, b: 1.37970e+03\n",
      "Iteration    9: Cost 4.19e+12  dj_dw:  2.882e+07, dj_db:  1.034e+06   w: -1.057e+05, b:-3.79030e+03\n"
     ]
    }
   ],
   "source": [
    "# initialize parameters\n",
    "w_init = 0\n",
    "b_init = 0\n",
    "# set alpha to a large value\n",
    "iterations = 10\n",
    "tmp_alpha = 5.0e-3\n",
    "\n",
    "# call gradient descent\n",
    "w_final, b_final, J_hist, p_hist = gradient_descent(x_train, y_train, w_init, b_init, \n",
    "                                                    tmp_alpha, iterations, compute_cost, compute_gradient)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e98781",
   "metadata": {},
   "source": [
    "Above, $w$ and $b$ are bouncing back and forth between positive and negative with the absolute value increasing with each iteration. Further, each iteration $\\frac{\\partial J(w,b)}{\\partial w}$ changes sign and cost is increasing rather than decreasing. This is a clear sign that the *learning rate is too large* and the solution is diverging. \n"
   ]
  }
 ],
 "metadata": {
  "dl_toc_settings": {
   "rndtag": "40291"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
